Overview
This project demonstrates the deployment and integration of LocalAI, a locally hosted large language model (LLM), as an AI Assistant. The goal is to provide a private and secure environment for AI operations, ensuring data privacy by avoiding exposure to external servers.

Key Features
Locally Hosted AI: Eliminates the need to rely on cloud-based AI solutions, ensuring sensitive data remains on the user's system.
Customizable Configuration: Supports various models, token limits, and adjustable parameters like temperature for response variability.
Streaming Responses: Implements chunked response streaming for low-latency interaction.
Full Control: Provides transparency and control over AI operations, addressing concerns around Shadow AI and data leakage.

how to start your local AI

docker compose up 

<img width="1668" height="930" alt="image" src="https://github.com/user-attachments/assets/42c17c71-1363-4051-82f9-684d17134fd4" />


Browse to your local AI

<img width="1661" height="875" alt="image" src="https://github.com/user-attachments/assets/07cf7eac-782f-4275-817d-698974b0589f" />


